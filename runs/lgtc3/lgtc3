
>> Initializing Linguateca dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Linguateca dataset


>> Building char dict...
>>> Started extracting chars from Linguateca dataset
<<< Finished extracting chars from Linguateca dataset
<< Finished building dicts!


>> Started preparing Linguateca dataset
<< Finished preparing Linguateca dataset

=================================================================
Linguateca Dataset
Train dataset #sents: 3332 #words: 64241
Val dataset #sents: 456 #words: 8064
Test dataset #sents: 424 #words: 8073
Tag set: [BOS, EOS, adj, adv, art, conj-c, conj-s, ec, intj, n, n-adj, n:, np, num, pp, pron-det, pron-indp, pron-pers, prop, prp, punct, v-fin, v-ger, v-inf, v-pcp]
=================================================================

POSTagger(
  (charBILSTM): CharBILSTM(
    (char_embeddings_table): Embedding(114, 70, padding_idx=0)
    (bilstm): LSTM(70, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.1)
  )
  (wordBILSTM1): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (wordBILSTM2): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (tag_bilstm): LSTM(350, 150, batch_first=True, bidirectional=True)
  (classifiers): ModuleList(
    (0): Linear(in_features=300, out_features=25, bias=True)
  )
  (dropout): Dropout(p=0.4)
)
train: batch_size=32, policy=visconde: 100% 104/104 [00:41<00:00,  2.38it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.22it/s]

=======================================================================================
Epoch: 0 	 Learning Rate: 1.000	Total Training Loss: 607.265945 	Total Validation Loss: 1114.812765 	 Duration: 45.275
>> Dataset Linguateca:	Training Loss: 607.265945	Validation Loss:1114.812765
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (inf --> 1114.812765).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.76it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.39it/s]

=======================================================================================
Epoch: 1 	 Learning Rate: 1.000	Total Training Loss: 376.388155 	Total Validation Loss: 786.751157 	 Duration: 44.092
>> Dataset Linguateca:	Training Loss: 376.388155	Validation Loss:786.751157
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (1114.812765 --> 786.751157).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.93it/s]

=======================================================================================
Epoch: 2 	 Learning Rate: 1.000	Total Training Loss: 283.044109 	Total Validation Loss: 634.333809 	 Duration: 44.123
>> Dataset Linguateca:	Training Loss: 283.044109	Validation Loss:634.333809
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (786.751157 --> 634.333809).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.46it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.29it/s]

=======================================================================================
Epoch: 3 	 Learning Rate: 1.000	Total Training Loss: 239.260931 	Total Validation Loss: 532.928646 	 Duration: 44.247
>> Dataset Linguateca:	Training Loss: 239.260931	Validation Loss:532.928646
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (634.333809 --> 532.928646).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:41<00:00,  2.80it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.63it/s]

=======================================================================================
Epoch: 4 	 Learning Rate: 1.000	Total Training Loss: 203.728515 	Total Validation Loss: 451.445995 	 Duration: 44.729
>> Dataset Linguateca:	Training Loss: 203.728515	Validation Loss:451.445995
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (532.928646 --> 451.445995).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.61it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.01it/s]

=======================================================================================
Epoch: 5 	 Learning Rate: 1.000	Total Training Loss: 175.654057 	Total Validation Loss: 409.789165 	 Duration: 44.388
>> Dataset Linguateca:	Training Loss: 175.654057	Validation Loss:409.789165
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (451.445995 --> 409.789165).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.53it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 123.70it/s]

=======================================================================================
Epoch: 6 	 Learning Rate: 1.000	Total Training Loss: 157.845808 	Total Validation Loss: 350.855552 	 Duration: 44.233
>> Dataset Linguateca:	Training Loss: 157.845808	Validation Loss:350.855552
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (409.789165 --> 350.855552).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.72it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.83it/s]

=======================================================================================
Epoch: 7 	 Learning Rate: 1.000	Total Training Loss: 140.083487 	Total Validation Loss: 311.840127 	 Duration: 44.476
>> Dataset Linguateca:	Training Loss: 140.083487	Validation Loss:311.840127
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (350.855552 --> 311.840127).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.54it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.37it/s]

=======================================================================================
Epoch: 8 	 Learning Rate: 1.000	Total Training Loss: 126.539443 	Total Validation Loss: 276.412979 	 Duration: 44.013
>> Dataset Linguateca:	Training Loss: 126.539443	Validation Loss:276.412979
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (311.840127 --> 276.412979).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.07it/s]

=======================================================================================
Epoch: 9 	 Learning Rate: 1.000	Total Training Loss: 115.745876 	Total Validation Loss: 257.777861 	 Duration: 44.060
>> Dataset Linguateca:	Training Loss: 115.745876	Validation Loss:257.777861
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (276.412979 --> 257.777861).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.07it/s]

=======================================================================================
Epoch: 10 	 Learning Rate: 1.000	Total Training Loss: 107.217438 	Total Validation Loss: 260.347128 	 Duration: 43.674
>> Dataset Linguateca:	Training Loss: 107.217438	Validation Loss:260.347128
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.62it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.82it/s]

=======================================================================================
Epoch: 11 	 Learning Rate: 1.000	Total Training Loss: 100.102388 	Total Validation Loss: 217.007927 	 Duration: 43.850
>> Dataset Linguateca:	Training Loss: 100.102388	Validation Loss:217.007927
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (257.777861 --> 217.007927).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 123.91it/s]

=======================================================================================
Epoch: 12 	 Learning Rate: 1.000	Total Training Loss: 93.292432 	Total Validation Loss: 204.586630 	 Duration: 43.892
>> Dataset Linguateca:	Training Loss: 93.292432	Validation Loss:204.586630
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (217.007927 --> 204.586630).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.33it/s]

=======================================================================================
Epoch: 13 	 Learning Rate: 1.000	Total Training Loss: 87.321334 	Total Validation Loss: 196.324824 	 Duration: 44.116
>> Dataset Linguateca:	Training Loss: 87.321334	Validation Loss:196.324824
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (204.586630 --> 196.324824).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.62it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 121.71it/s]

=======================================================================================
Epoch: 14 	 Learning Rate: 1.000	Total Training Loss: 83.122869 	Total Validation Loss: 177.644938 	 Duration: 44.277
>> Dataset Linguateca:	Training Loss: 83.122869	Validation Loss:177.644938
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (196.324824 --> 177.644938).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.48it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.44it/s]

=======================================================================================
Epoch: 15 	 Learning Rate: 1.000	Total Training Loss: 77.369958 	Total Validation Loss: 165.451911 	 Duration: 44.034
>> Dataset Linguateca:	Training Loss: 77.369958	Validation Loss:165.451911
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (177.644938 --> 165.451911).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.53it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.39it/s]

=======================================================================================
Epoch: 16 	 Learning Rate: 1.000	Total Training Loss: 72.796853 	Total Validation Loss: 157.412454 	 Duration: 43.929
>> Dataset Linguateca:	Training Loss: 72.796853	Validation Loss:157.412454
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (165.451911 --> 157.412454).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:41<00:00,  2.70it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 123.33it/s]

=======================================================================================
Epoch: 17 	 Learning Rate: 1.000	Total Training Loss: 68.509647 	Total Validation Loss: 193.858992 	 Duration: 45.442
>> Dataset Linguateca:	Training Loss: 68.509647	Validation Loss:193.858992
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.61it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 121.79it/s]

=======================================================================================
Epoch: 18 	 Learning Rate: 1.000	Total Training Loss: 65.697207 	Total Validation Loss: 145.932791 	 Duration: 44.422
>> Dataset Linguateca:	Training Loss: 65.697207	Validation Loss:145.932791
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (157.412454 --> 145.932791).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.39it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.39it/s]

=======================================================================================
Epoch: 19 	 Learning Rate: 1.000	Total Training Loss: 62.302094 	Total Validation Loss: 132.310330 	 Duration: 44.096
>> Dataset Linguateca:	Training Loss: 62.302094	Validation Loss:132.310330
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (145.932791 --> 132.310330).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.00it/s]

=======================================================================================
Epoch: 20 	 Learning Rate: 1.000	Total Training Loss: 59.608500 	Total Validation Loss: 129.481124 	 Duration: 44.192
>> Dataset Linguateca:	Training Loss: 59.608500	Validation Loss:129.481124
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (132.310330 --> 129.481124).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.70it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.05it/s]

=======================================================================================
Epoch: 21 	 Learning Rate: 1.000	Total Training Loss: 57.246519 	Total Validation Loss: 127.653189 	 Duration: 44.540
>> Dataset Linguateca:	Training Loss: 57.246519	Validation Loss:127.653189
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (129.481124 --> 127.653189).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.63it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.59it/s]

=======================================================================================
Epoch: 22 	 Learning Rate: 1.000	Total Training Loss: 54.208535 	Total Validation Loss: 119.969328 	 Duration: 44.575
>> Dataset Linguateca:	Training Loss: 54.208535	Validation Loss:119.969328
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (127.653189 --> 119.969328).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.52it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.95it/s]

=======================================================================================
Epoch: 23 	 Learning Rate: 1.000	Total Training Loss: 52.825230 	Total Validation Loss: 118.337600 	 Duration: 44.481
>> Dataset Linguateca:	Training Loss: 52.825230	Validation Loss:118.337600
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (119.969328 --> 118.337600).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.61it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.23it/s]

=======================================================================================
Epoch: 24 	 Learning Rate: 1.000	Total Training Loss: 50.644265 	Total Validation Loss: 120.712405 	 Duration: 43.873
>> Dataset Linguateca:	Training Loss: 50.644265	Validation Loss:120.712405
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.52it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.38it/s]

=======================================================================================
Epoch: 25 	 Learning Rate: 1.000	Total Training Loss: 47.873014 	Total Validation Loss: 109.734827 	 Duration: 43.923
>> Dataset Linguateca:	Training Loss: 47.873014	Validation Loss:109.734827
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (118.337600 --> 109.734827).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.79it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.28it/s]

=======================================================================================
Epoch: 26 	 Learning Rate: 1.000	Total Training Loss: 46.916127 	Total Validation Loss: 107.403235 	 Duration: 44.044
>> Dataset Linguateca:	Training Loss: 46.916127	Validation Loss:107.403235
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (109.734827 --> 107.403235).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.04it/s]

=======================================================================================
Epoch: 27 	 Learning Rate: 1.000	Total Training Loss: 45.275421 	Total Validation Loss: 103.255754 	 Duration: 43.944
>> Dataset Linguateca:	Training Loss: 45.275421	Validation Loss:103.255754
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (107.403235 --> 103.255754).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.51it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 123.19it/s]

=======================================================================================
Epoch: 28 	 Learning Rate: 1.000	Total Training Loss: 43.558755 	Total Validation Loss: 100.616004 	 Duration: 44.338
>> Dataset Linguateca:	Training Loss: 43.558755	Validation Loss:100.616004
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (103.255754 --> 100.616004).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.48it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.55it/s]

=======================================================================================
Epoch: 29 	 Learning Rate: 1.000	Total Training Loss: 41.361805 	Total Validation Loss: 97.973716 	 Duration: 44.008
>> Dataset Linguateca:	Training Loss: 41.361805	Validation Loss:97.973716
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (100.616004 --> 97.973716).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.74it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.92it/s]

=======================================================================================
Epoch: 30 	 Learning Rate: 1.000	Total Training Loss: 40.368040 	Total Validation Loss: 97.449393 	 Duration: 43.659
>> Dataset Linguateca:	Training Loss: 40.368040	Validation Loss:97.449393
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (97.973716 --> 97.449393).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.02it/s]

=======================================================================================
Epoch: 31 	 Learning Rate: 1.000	Total Training Loss: 39.281579 	Total Validation Loss: 93.578399 	 Duration: 43.350
>> Dataset Linguateca:	Training Loss: 39.281579	Validation Loss:93.578399
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (97.449393 --> 93.578399).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.00it/s]

=======================================================================================
Epoch: 32 	 Learning Rate: 1.000	Total Training Loss: 37.826722 	Total Validation Loss: 89.048784 	 Duration: 43.215
>> Dataset Linguateca:	Training Loss: 37.826722	Validation Loss:89.048784
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (93.578399 --> 89.048784).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.65it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 128.01it/s]

=======================================================================================
Epoch: 33 	 Learning Rate: 1.000	Total Training Loss: 36.992011 	Total Validation Loss: 88.339345 	 Duration: 43.138
>> Dataset Linguateca:	Training Loss: 36.992011	Validation Loss:88.339345
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (89.048784 --> 88.339345).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.26it/s]

=======================================================================================
Epoch: 34 	 Learning Rate: 1.000	Total Training Loss: 35.405515 	Total Validation Loss: 86.823864 	 Duration: 43.233
>> Dataset Linguateca:	Training Loss: 35.405515	Validation Loss:86.823864
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (88.339345 --> 86.823864).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.68it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.60it/s]

=======================================================================================
Epoch: 35 	 Learning Rate: 1.000	Total Training Loss: 34.567196 	Total Validation Loss: 86.176360 	 Duration: 43.308
>> Dataset Linguateca:	Training Loss: 34.567196	Validation Loss:86.176360
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (86.823864 --> 86.176360).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.59it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 119.61it/s]

=======================================================================================
Epoch: 36 	 Learning Rate: 1.000	Total Training Loss: 33.350311 	Total Validation Loss: 82.320697 	 Duration: 43.296
>> Dataset Linguateca:	Training Loss: 33.350311	Validation Loss:82.320697
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (86.176360 --> 82.320697).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.72it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.89it/s]

=======================================================================================
Epoch: 37 	 Learning Rate: 1.000	Total Training Loss: 32.920780 	Total Validation Loss: 80.804566 	 Duration: 42.972
>> Dataset Linguateca:	Training Loss: 32.920780	Validation Loss:80.804566
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (82.320697 --> 80.804566).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.50it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.04it/s]

=======================================================================================
Epoch: 38 	 Learning Rate: 1.000	Total Training Loss: 31.965858 	Total Validation Loss: 79.616199 	 Duration: 43.217
>> Dataset Linguateca:	Training Loss: 31.965858	Validation Loss:79.616199
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (80.804566 --> 79.616199).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.49it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 120.18it/s]

=======================================================================================
Epoch: 39 	 Learning Rate: 1.000	Total Training Loss: 30.097877 	Total Validation Loss: 77.690132 	 Duration: 43.262
>> Dataset Linguateca:	Training Loss: 30.097877	Validation Loss:77.690132
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (79.616199 --> 77.690132).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.30it/s]

=======================================================================================
Epoch: 40 	 Learning Rate: 1.000	Total Training Loss: 29.780673 	Total Validation Loss: 78.346551 	 Duration: 43.464
>> Dataset Linguateca:	Training Loss: 29.780673	Validation Loss:78.346551
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.47it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 128.58it/s]

=======================================================================================
Epoch: 41 	 Learning Rate: 1.000	Total Training Loss: 29.216578 	Total Validation Loss: 75.777363 	 Duration: 42.854
>> Dataset Linguateca:	Training Loss: 29.216578	Validation Loss:75.777363
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (77.690132 --> 75.777363).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.74it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.08it/s]

=======================================================================================
Epoch: 42 	 Learning Rate: 1.000	Total Training Loss: 28.721125 	Total Validation Loss: 85.323737 	 Duration: 43.574
>> Dataset Linguateca:	Training Loss: 28.721125	Validation Loss:85.323737
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.64it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 125.67it/s]

=======================================================================================
Epoch: 43 	 Learning Rate: 1.000	Total Training Loss: 27.687213 	Total Validation Loss: 72.023955 	 Duration: 44.043
>> Dataset Linguateca:	Training Loss: 27.687213	Validation Loss:72.023955
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (75.777363 --> 72.023955).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.57it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 122.12it/s]

=======================================================================================
Epoch: 44 	 Learning Rate: 1.000	Total Training Loss: 27.321652 	Total Validation Loss: 78.496721 	 Duration: 43.881
>> Dataset Linguateca:	Training Loss: 27.321652	Validation Loss:78.496721
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:40<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 124.78it/s]

=======================================================================================
Epoch: 45 	 Learning Rate: 1.000	Total Training Loss: 26.281554 	Total Validation Loss: 79.848386 	 Duration: 43.673
>> Dataset Linguateca:	Training Loss: 26.281554	Validation Loss:79.848386
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.59it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.08it/s]

=======================================================================================
Epoch: 46 	 Learning Rate: 1.000	Total Training Loss: 26.018604 	Total Validation Loss: 73.320256 	 Duration: 43.485
>> Dataset Linguateca:	Training Loss: 26.018604	Validation Loss:73.320256
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.57it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 126.29it/s]

=======================================================================================
Epoch: 47 	 Learning Rate: 1.000	Total Training Loss: 25.467719 	Total Validation Loss: 76.314902 	 Duration: 43.400
>> Dataset Linguateca:	Training Loss: 25.467719	Validation Loss:76.314902
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.90it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 129.86it/s]

=======================================================================================
Epoch: 48 	 Learning Rate: 1.000	Total Training Loss: 25.177103 	Total Validation Loss: 85.581896 	 Duration: 42.634
>> Dataset Linguateca:	Training Loss: 25.177103	Validation Loss:85.581896
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.59it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.46it/s]

=======================================================================================
Epoch: 49 	 Learning Rate: 1.000	Total Training Loss: 23.972779 	Total Validation Loss: 70.801139 	 Duration: 42.302
>> Dataset Linguateca:	Training Loss: 23.972779	Validation Loss:70.801139
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (72.023955 --> 70.801139).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.54it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 127.77it/s]

=======================================================================================
Epoch: 50 	 Learning Rate: 1.000	Total Training Loss: 23.427580 	Total Validation Loss: 75.388087 	 Duration: 42.565
>> Dataset Linguateca:	Training Loss: 23.427580	Validation Loss:75.388087
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.72it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 131.93it/s]

=======================================================================================
Epoch: 51 	 Learning Rate: 1.000	Total Training Loss: 23.000450 	Total Validation Loss: 71.423141 	 Duration: 41.807
>> Dataset Linguateca:	Training Loss: 23.000450	Validation Loss:71.423141
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 132.19it/s]

=======================================================================================
Epoch: 52 	 Learning Rate: 1.000	Total Training Loss: 22.517404 	Total Validation Loss: 74.557775 	 Duration: 41.432
>> Dataset Linguateca:	Training Loss: 22.517404	Validation Loss:74.557775
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.90it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 132.41it/s]

=======================================================================================
Epoch: 53 	 Learning Rate: 1.000	Total Training Loss: 22.411453 	Total Validation Loss: 68.496777 	 Duration: 41.425
>> Dataset Linguateca:	Training Loss: 22.411453	Validation Loss:68.496777
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (70.801139 --> 68.496777).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.76it/s]
val: batch_size=1, policy=emilia: 100% 456/456 [00:03<00:00, 132.27it/s]

=======================================================================================
Epoch: 54 	 Learning Rate: 1.000	Total Training Loss: 21.636211 	Total Validation Loss: 68.094823 	 Duration: 41.423
>> Dataset Linguateca:	Training Loss: 21.636211	Validation Loss:68.094823
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (68.496777 --> 68.094823).  Saving model ...
=======================================================================================
test: batch_size=1, policy=emilia: 100% 424/424 [00:03<00:00, 112.05it/s]

Test Accuracy (Overall): 96% (7769/8072)

Test Accuracy (on Linguateca Dataset): 96.25% (7769/8072)
