
>> Initializing Macmorpho dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Macmorpho dataset


>> Initializing Bosque dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Bosque dataset


>> Initializing GSD dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing GSD dataset


>> Initializing Linguateca dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Linguateca dataset


>> Building char dict...
>>> Started extracting chars from Linguateca dataset
<<< Finished extracting chars from Linguateca dataset
<< Finished building dicts!


>> Started preparing Linguateca dataset
<< Finished preparing Linguateca dataset

=================================================================
Linguateca Dataset
Train dataset #sents: 3355 #words: 65086
Val dataset #sents: 419 #words: 7229
Test dataset #sents: 420 #words: 7995
Tag set: [BOS, EOS, adj, adv, art, conj-c, conj-s, ec, intj, n, n-adj, n:, num, pron-det, pron-indp, pron-pers, prop, prp, punct, v-fin, v-ger, v-inf, v-pcp, vp]
=================================================================

POSTagger(
  (charBILSTM): CharBILSTM(
    (char_embeddings_table): Embedding(115, 70, padding_idx=0)
    (bilstm): LSTM(70, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.1)
  )
  (wordBILSTM1): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (wordBILSTM2): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (tag_bilstm): LSTM(350, 150, batch_first=True, bidirectional=True)
  (classifiers): ModuleList(
    (0): Linear(in_features=300, out_features=24, bias=True)
  )
  (dropout): Dropout(p=0.4)
)
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.96it/s]

=======================================================================================
Epoch: 0 	 Learning Rate: 1.000	Total Training Loss: 178.285880 	Total Validation Loss: 1698.591443 	 Duration: 41.766
>> Dataset Linguateca:	Training Loss: 178.285880	Validation Loss:1698.591443
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (inf --> 1698.591443).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.69it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.86it/s]

=======================================================================================
Epoch: 1 	 Learning Rate: 1.000	Total Training Loss: 111.128725 	Total Validation Loss: 924.080888 	 Duration: 41.541
>> Dataset Linguateca:	Training Loss: 111.128725	Validation Loss:924.080888
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (1698.591443 --> 924.080888).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.55it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.41it/s]

=======================================================================================
Epoch: 2 	 Learning Rate: 1.000	Total Training Loss: 83.771793 	Total Validation Loss: 752.714070 	 Duration: 41.463
>> Dataset Linguateca:	Training Loss: 83.771793	Validation Loss:752.714070
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (924.080888 --> 752.714070).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.80it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.96it/s]

=======================================================================================
Epoch: 3 	 Learning Rate: 1.000	Total Training Loss: 68.169802 	Total Validation Loss: 626.542974 	 Duration: 41.133
>> Dataset Linguateca:	Training Loss: 68.169802	Validation Loss:626.542974
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (752.714070 --> 626.542974).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.71it/s]

=======================================================================================
Epoch: 4 	 Learning Rate: 1.000	Total Training Loss: 57.844572 	Total Validation Loss: 534.548408 	 Duration: 41.165
>> Dataset Linguateca:	Training Loss: 57.844572	Validation Loss:534.548408
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (626.542974 --> 534.548408).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.05it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.10it/s]

=======================================================================================
Epoch: 5 	 Learning Rate: 1.000	Total Training Loss: 51.084009 	Total Validation Loss: 480.136168 	 Duration: 40.751
>> Dataset Linguateca:	Training Loss: 51.084009	Validation Loss:480.136168
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (534.548408 --> 480.136168).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.88it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.47it/s]

=======================================================================================
Epoch: 6 	 Learning Rate: 1.000	Total Training Loss: 45.916635 	Total Validation Loss: 423.821893 	 Duration: 40.762
>> Dataset Linguateca:	Training Loss: 45.916635	Validation Loss:423.821893
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (480.136168 --> 423.821893).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.71it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.32it/s]

=======================================================================================
Epoch: 7 	 Learning Rate: 1.000	Total Training Loss: 41.432141 	Total Validation Loss: 385.241386 	 Duration: 40.756
>> Dataset Linguateca:	Training Loss: 41.432141	Validation Loss:385.241386
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (423.821893 --> 385.241386).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.81it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.18it/s]

=======================================================================================
Epoch: 8 	 Learning Rate: 1.000	Total Training Loss: 37.534916 	Total Validation Loss: 348.604842 	 Duration: 40.783
>> Dataset Linguateca:	Training Loss: 37.534916	Validation Loss:348.604842
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (385.241386 --> 348.604842).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.79it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.89it/s]

=======================================================================================
Epoch: 9 	 Learning Rate: 1.000	Total Training Loss: 33.993278 	Total Validation Loss: 325.671129 	 Duration: 40.628
>> Dataset Linguateca:	Training Loss: 33.993278	Validation Loss:325.671129
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (348.604842 --> 325.671129).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 116.28it/s]

=======================================================================================
Epoch: 10 	 Learning Rate: 1.000	Total Training Loss: 31.281855 	Total Validation Loss: 282.372138 	 Duration: 40.905
>> Dataset Linguateca:	Training Loss: 31.281855	Validation Loss:282.372138
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (325.671129 --> 282.372138).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.47it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.03it/s]

=======================================================================================
Epoch: 11 	 Learning Rate: 1.000	Total Training Loss: 28.937461 	Total Validation Loss: 263.512229 	 Duration: 40.899
>> Dataset Linguateca:	Training Loss: 28.937461	Validation Loss:263.512229
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (282.372138 --> 263.512229).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.74it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.07it/s]

=======================================================================================
Epoch: 12 	 Learning Rate: 1.000	Total Training Loss: 26.999946 	Total Validation Loss: 242.145235 	 Duration: 40.704
>> Dataset Linguateca:	Training Loss: 26.999946	Validation Loss:242.145235
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (263.512229 --> 242.145235).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.78it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.51it/s]

=======================================================================================
Epoch: 13 	 Learning Rate: 1.000	Total Training Loss: 25.368028 	Total Validation Loss: 224.887188 	 Duration: 40.635
>> Dataset Linguateca:	Training Loss: 25.368028	Validation Loss:224.887188
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (242.145235 --> 224.887188).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.98it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.24it/s]

=======================================================================================
Epoch: 14 	 Learning Rate: 1.000	Total Training Loss: 23.917363 	Total Validation Loss: 214.291450 	 Duration: 40.723
>> Dataset Linguateca:	Training Loss: 23.917363	Validation Loss:214.291450
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (224.887188 --> 214.291450).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.97it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.64it/s]

=======================================================================================
Epoch: 15 	 Learning Rate: 1.000	Total Training Loss: 22.557842 	Total Validation Loss: 198.214942 	 Duration: 40.596
>> Dataset Linguateca:	Training Loss: 22.557842	Validation Loss:198.214942
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (214.291450 --> 198.214942).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.83it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.60it/s]

=======================================================================================
Epoch: 16 	 Learning Rate: 1.000	Total Training Loss: 21.396299 	Total Validation Loss: 188.388817 	 Duration: 40.812
>> Dataset Linguateca:	Training Loss: 21.396299	Validation Loss:188.388817
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (198.214942 --> 188.388817).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.80it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.33it/s]

=======================================================================================
Epoch: 17 	 Learning Rate: 1.000	Total Training Loss: 20.435870 	Total Validation Loss: 181.607957 	 Duration: 41.048
>> Dataset Linguateca:	Training Loss: 20.435870	Validation Loss:181.607957
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (188.388817 --> 181.607957).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.89it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.65it/s]

=======================================================================================
Epoch: 18 	 Learning Rate: 1.000	Total Training Loss: 19.392667 	Total Validation Loss: 173.526549 	 Duration: 41.136
>> Dataset Linguateca:	Training Loss: 19.392667	Validation Loss:173.526549
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (181.607957 --> 173.526549).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.99it/s]

=======================================================================================
Epoch: 19 	 Learning Rate: 1.000	Total Training Loss: 18.534989 	Total Validation Loss: 159.099314 	 Duration: 40.806
>> Dataset Linguateca:	Training Loss: 18.534989	Validation Loss:159.099314
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (173.526549 --> 159.099314).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.00it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.78it/s]

=======================================================================================
Epoch: 20 	 Learning Rate: 1.000	Total Training Loss: 17.677420 	Total Validation Loss: 157.849840 	 Duration: 40.671
>> Dataset Linguateca:	Training Loss: 17.677420	Validation Loss:157.849840
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (159.099314 --> 157.849840).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.83it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.36it/s]

=======================================================================================
Epoch: 21 	 Learning Rate: 1.000	Total Training Loss: 16.654182 	Total Validation Loss: 153.909234 	 Duration: 40.535
>> Dataset Linguateca:	Training Loss: 16.654182	Validation Loss:153.909234
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (157.849840 --> 153.909234).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.02it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.35it/s]

=======================================================================================
Epoch: 22 	 Learning Rate: 1.000	Total Training Loss: 16.182965 	Total Validation Loss: 145.702967 	 Duration: 40.673
>> Dataset Linguateca:	Training Loss: 16.182965	Validation Loss:145.702967
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (153.909234 --> 145.702967).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.87it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.86it/s]

=======================================================================================
Epoch: 23 	 Learning Rate: 1.000	Total Training Loss: 15.504274 	Total Validation Loss: 142.597099 	 Duration: 40.555
>> Dataset Linguateca:	Training Loss: 15.504274	Validation Loss:142.597099
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (145.702967 --> 142.597099).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.31it/s]

=======================================================================================
Epoch: 24 	 Learning Rate: 1.000	Total Training Loss: 14.928070 	Total Validation Loss: 133.510407 	 Duration: 40.784
>> Dataset Linguateca:	Training Loss: 14.928070	Validation Loss:133.510407
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (142.597099 --> 133.510407).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.02it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.02it/s]

=======================================================================================
Epoch: 25 	 Learning Rate: 1.000	Total Training Loss: 14.396977 	Total Validation Loss: 131.774827 	 Duration: 40.669
>> Dataset Linguateca:	Training Loss: 14.396977	Validation Loss:131.774827
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (133.510407 --> 131.774827).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  3.06it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.45it/s]

=======================================================================================
Epoch: 26 	 Learning Rate: 1.000	Total Training Loss: 13.909015 	Total Validation Loss: 133.461749 	 Duration: 41.284
>> Dataset Linguateca:	Training Loss: 13.909015	Validation Loss:133.461749
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.57it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 118.87it/s]

=======================================================================================
Epoch: 27 	 Learning Rate: 1.000	Total Training Loss: 13.304723 	Total Validation Loss: 126.905284 	 Duration: 40.844
>> Dataset Linguateca:	Training Loss: 13.304723	Validation Loss:126.905284
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (131.774827 --> 126.905284).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.49it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.42it/s]

=======================================================================================
Epoch: 28 	 Learning Rate: 1.000	Total Training Loss: 13.103217 	Total Validation Loss: 123.191332 	 Duration: 40.988
>> Dataset Linguateca:	Training Loss: 13.103217	Validation Loss:123.191332
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (126.905284 --> 123.191332).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.01it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.56it/s]

=======================================================================================
Epoch: 29 	 Learning Rate: 1.000	Total Training Loss: 12.540634 	Total Validation Loss: 118.033685 	 Duration: 40.579
>> Dataset Linguateca:	Training Loss: 12.540634	Validation Loss:118.033685
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (123.191332 --> 118.033685).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.20it/s]

=======================================================================================
Epoch: 30 	 Learning Rate: 1.000	Total Training Loss: 12.062197 	Total Validation Loss: 125.644945 	 Duration: 40.544
>> Dataset Linguateca:	Training Loss: 12.062197	Validation Loss:125.644945
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.88it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.96it/s]

=======================================================================================
Epoch: 31 	 Learning Rate: 1.000	Total Training Loss: 11.716443 	Total Validation Loss: 116.227119 	 Duration: 40.796
>> Dataset Linguateca:	Training Loss: 11.716443	Validation Loss:116.227119
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (118.033685 --> 116.227119).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.46it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.96it/s]

=======================================================================================
Epoch: 32 	 Learning Rate: 1.000	Total Training Loss: 11.242144 	Total Validation Loss: 113.682235 	 Duration: 40.536
>> Dataset Linguateca:	Training Loss: 11.242144	Validation Loss:113.682235
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (116.227119 --> 113.682235).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.76it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.05it/s]

=======================================================================================
Epoch: 33 	 Learning Rate: 1.000	Total Training Loss: 11.044266 	Total Validation Loss: 110.905551 	 Duration: 40.650
>> Dataset Linguateca:	Training Loss: 11.044266	Validation Loss:110.905551
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (113.682235 --> 110.905551).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.59it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.92it/s]

=======================================================================================
Epoch: 34 	 Learning Rate: 1.000	Total Training Loss: 10.791525 	Total Validation Loss: 112.453157 	 Duration: 40.608
>> Dataset Linguateca:	Training Loss: 10.791525	Validation Loss:112.453157
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.70it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.52it/s]

=======================================================================================
Epoch: 35 	 Learning Rate: 1.000	Total Training Loss: 10.325950 	Total Validation Loss: 104.670850 	 Duration: 40.409
>> Dataset Linguateca:	Training Loss: 10.325950	Validation Loss:104.670850
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (110.905551 --> 104.670850).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.94it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 137.01it/s]

=======================================================================================
Epoch: 36 	 Learning Rate: 1.000	Total Training Loss: 10.112500 	Total Validation Loss: 107.778323 	 Duration: 40.576
>> Dataset Linguateca:	Training Loss: 10.112500	Validation Loss:107.778323
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.62it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.72it/s]

=======================================================================================
Epoch: 37 	 Learning Rate: 1.000	Total Training Loss: 9.822345 	Total Validation Loss: 102.813010 	 Duration: 40.566
>> Dataset Linguateca:	Training Loss: 9.822345	Validation Loss:102.813010
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (104.670850 --> 102.813010).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.25it/s]

=======================================================================================
Epoch: 38 	 Learning Rate: 1.000	Total Training Loss: 9.534755 	Total Validation Loss: 109.769861 	 Duration: 40.487
>> Dataset Linguateca:	Training Loss: 9.534755	Validation Loss:109.769861
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.57it/s]

=======================================================================================
Epoch: 39 	 Learning Rate: 1.000	Total Training Loss: 9.294651 	Total Validation Loss: 105.441677 	 Duration: 40.523
>> Dataset Linguateca:	Training Loss: 9.294651	Validation Loss:105.441677
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.90it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 135.02it/s]

=======================================================================================
Epoch: 40 	 Learning Rate: 1.000	Total Training Loss: 8.970865 	Total Validation Loss: 100.201849 	 Duration: 40.524
>> Dataset Linguateca:	Training Loss: 8.970865	Validation Loss:100.201849
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (102.813010 --> 100.201849).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.62it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 116.27it/s]

=======================================================================================
Epoch: 41 	 Learning Rate: 1.000	Total Training Loss: 8.924298 	Total Validation Loss: 100.320615 	 Duration: 40.880
>> Dataset Linguateca:	Training Loss: 8.924298	Validation Loss:100.320615
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.00it/s]

=======================================================================================
Epoch: 42 	 Learning Rate: 1.000	Total Training Loss: 8.531278 	Total Validation Loss: 99.367402 	 Duration: 40.481
>> Dataset Linguateca:	Training Loss: 8.531278	Validation Loss:99.367402
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (100.201849 --> 99.367402).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.05it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.90it/s]

=======================================================================================
Epoch: 43 	 Learning Rate: 1.000	Total Training Loss: 8.354172 	Total Validation Loss: 97.494488 	 Duration: 40.418
>> Dataset Linguateca:	Training Loss: 8.354172	Validation Loss:97.494488
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (99.367402 --> 97.494488).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.91it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.29it/s]

=======================================================================================
Epoch: 44 	 Learning Rate: 1.000	Total Training Loss: 8.174002 	Total Validation Loss: 94.670868 	 Duration: 40.478
>> Dataset Linguateca:	Training Loss: 8.174002	Validation Loss:94.670868
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (97.494488 --> 94.670868).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.99it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.10it/s]

=======================================================================================
Epoch: 45 	 Learning Rate: 1.000	Total Training Loss: 7.984338 	Total Validation Loss: 91.822004 	 Duration: 40.538
>> Dataset Linguateca:	Training Loss: 7.984338	Validation Loss:91.822004
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (94.670868 --> 91.822004).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.42it/s]

=======================================================================================
Epoch: 46 	 Learning Rate: 1.000	Total Training Loss: 7.686787 	Total Validation Loss: 94.578328 	 Duration: 40.430
>> Dataset Linguateca:	Training Loss: 7.686787	Validation Loss:94.578328
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.72it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.72it/s]

=======================================================================================
Epoch: 47 	 Learning Rate: 1.000	Total Training Loss: 7.609846 	Total Validation Loss: 98.557413 	 Duration: 40.490
>> Dataset Linguateca:	Training Loss: 7.609846	Validation Loss:98.557413
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.23it/s]

=======================================================================================
Epoch: 48 	 Learning Rate: 1.000	Total Training Loss: 7.412585 	Total Validation Loss: 90.665873 	 Duration: 40.455
>> Dataset Linguateca:	Training Loss: 7.412585	Validation Loss:90.665873
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (91.822004 --> 90.665873).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.13it/s]

=======================================================================================
Epoch: 49 	 Learning Rate: 1.000	Total Training Loss: 7.081261 	Total Validation Loss: 91.632515 	 Duration: 40.862
>> Dataset Linguateca:	Training Loss: 7.081261	Validation Loss:91.632515
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.02it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.02it/s]

=======================================================================================
Epoch: 50 	 Learning Rate: 1.000	Total Training Loss: 7.152731 	Total Validation Loss: 91.436288 	 Duration: 40.378
>> Dataset Linguateca:	Training Loss: 7.152731	Validation Loss:91.436288
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.96it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.46it/s]

=======================================================================================
Epoch: 51 	 Learning Rate: 1.000	Total Training Loss: 7.063761 	Total Validation Loss: 91.314457 	 Duration: 40.519
>> Dataset Linguateca:	Training Loss: 7.063761	Validation Loss:91.314457
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.58it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.19it/s]

=======================================================================================
Epoch: 52 	 Learning Rate: 1.000	Total Training Loss: 6.654917 	Total Validation Loss: 93.458491 	 Duration: 40.390
>> Dataset Linguateca:	Training Loss: 6.654917	Validation Loss:93.458491
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.71it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.70it/s]

=======================================================================================
Epoch: 53 	 Learning Rate: 1.000	Total Training Loss: 6.590675 	Total Validation Loss: 90.262431 	 Duration: 40.534
>> Dataset Linguateca:	Training Loss: 6.590675	Validation Loss:90.262431
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (90.665873 --> 90.262431).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.17it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 136.36it/s]

=======================================================================================
Epoch: 54 	 Learning Rate: 1.000	Total Training Loss: 6.523157 	Total Validation Loss: 100.174274 	 Duration: 40.424
>> Dataset Linguateca:	Training Loss: 6.523157	Validation Loss:100.174274
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
test: batch_size=1, policy=emilia: 100% 420/420 [00:03<00:00, 114.11it/s]

Test Accuracy (Overall): 96% (7694/7993)

Test Accuracy (on Linguateca Dataset): 96.26% (7694/7993)
