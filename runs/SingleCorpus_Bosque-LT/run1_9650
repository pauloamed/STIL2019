
>> Initializing Macmorpho dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Macmorpho dataset


>> Initializing Bosque dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Bosque dataset


>> Initializing GSD dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing GSD dataset


>> Initializing Linguateca dataset
>>> Started loading dataset
<<< Finished loading dataset
>>> Started parsing data from dataset
<<< Finished parsing data from dataset
>>> Started building tag dict for dataset
<<< Finished building tag dict for dataset
<< Finished initializing Linguateca dataset


>> Building char dict...
>>> Started extracting chars from Linguateca dataset
<<< Finished extracting chars from Linguateca dataset
<< Finished building dicts!


>> Started preparing Linguateca dataset
<< Finished preparing Linguateca dataset

=================================================================
Linguateca Dataset
Train dataset #sents: 3355 #words: 65086
Val dataset #sents: 419 #words: 7229
Test dataset #sents: 420 #words: 7995
Tag set: [BOS, EOS, adj, adv, art, conj-c, conj-s, ec, intj, n, n-adj, n:, num, pron-det, pron-indp, pron-pers, prop, prp, punct, v-fin, v-ger, v-inf, v-pcp, vp]
=================================================================

POSTagger(
  (charBILSTM): CharBILSTM(
    (char_embeddings_table): Embedding(115, 70, padding_idx=0)
    (bilstm): LSTM(70, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.1)
  )
  (wordBILSTM1): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (wordBILSTM2): WordBILSTM(
    (bilstm): LSTM(350, 350, batch_first=True, bidirectional=True)
    (projection_layer): Linear(in_features=700, out_features=350, bias=True)
    (dropout): Dropout(p=0.2)
  )
  (tag_bilstm): LSTM(350, 150, batch_first=True, bidirectional=True)
  (classifiers): ModuleList(
    (0): Linear(in_features=300, out_features=24, bias=True)
  )
  (dropout): Dropout(p=0.4)
)
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.70it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 129.71it/s]

=======================================================================================
Epoch: 0 	 Learning Rate: 1.000	Total Training Loss: 179.133841 	Total Validation Loss: 1512.167716 	 Duration: 42.490
>> Dataset Linguateca:	Training Loss: 179.133841	Validation Loss:1512.167716
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (inf --> 1512.167716).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.63it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 130.42it/s]

=======================================================================================
Epoch: 1 	 Learning Rate: 1.000	Total Training Loss: 110.438694 	Total Validation Loss: 892.194587 	 Duration: 42.488
>> Dataset Linguateca:	Training Loss: 110.438694	Validation Loss:892.194587
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (1512.167716 --> 892.194587).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.47it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 130.04it/s]

=======================================================================================
Epoch: 2 	 Learning Rate: 1.000	Total Training Loss: 78.633690 	Total Validation Loss: 716.050644 	 Duration: 41.998
>> Dataset Linguateca:	Training Loss: 78.633690	Validation Loss:716.050644
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (892.194587 --> 716.050644).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:39<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 130.71it/s]

=======================================================================================
Epoch: 3 	 Learning Rate: 1.000	Total Training Loss: 63.558896 	Total Validation Loss: 572.815968 	 Duration: 42.436
>> Dataset Linguateca:	Training Loss: 63.558896	Validation Loss:572.815968
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (716.050644 --> 572.815968).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.88it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.45it/s]

=======================================================================================
Epoch: 4 	 Learning Rate: 1.000	Total Training Loss: 53.947046 	Total Validation Loss: 502.620101 	 Duration: 41.764
>> Dataset Linguateca:	Training Loss: 53.947046	Validation Loss:502.620101
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (572.815968 --> 502.620101).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.40it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 130.80it/s]

=======================================================================================
Epoch: 5 	 Learning Rate: 1.000	Total Training Loss: 47.767132 	Total Validation Loss: 460.612881 	 Duration: 41.690
>> Dataset Linguateca:	Training Loss: 47.767132	Validation Loss:460.612881
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (502.620101 --> 460.612881).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.84it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 131.98it/s]

=======================================================================================
Epoch: 6 	 Learning Rate: 1.000	Total Training Loss: 42.364089 	Total Validation Loss: 407.320183 	 Duration: 41.824
>> Dataset Linguateca:	Training Loss: 42.364089	Validation Loss:407.320183
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (460.612881 --> 407.320183).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  3.09it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.71it/s]

=======================================================================================
Epoch: 7 	 Learning Rate: 1.000	Total Training Loss: 37.579858 	Total Validation Loss: 356.679793 	 Duration: 41.727
>> Dataset Linguateca:	Training Loss: 37.579858	Validation Loss:356.679793
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (407.320183 --> 356.679793).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.80it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 130.66it/s]

=======================================================================================
Epoch: 8 	 Learning Rate: 1.000	Total Training Loss: 34.472189 	Total Validation Loss: 309.095296 	 Duration: 41.395
>> Dataset Linguateca:	Training Loss: 34.472189	Validation Loss:309.095296
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (356.679793 --> 309.095296).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.63it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.40it/s]

=======================================================================================
Epoch: 9 	 Learning Rate: 1.000	Total Training Loss: 31.207003 	Total Validation Loss: 281.256340 	 Duration: 41.576
>> Dataset Linguateca:	Training Loss: 31.207003	Validation Loss:281.256340
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (309.095296 --> 281.256340).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 107.36it/s]

=======================================================================================
Epoch: 10 	 Learning Rate: 1.000	Total Training Loss: 28.727354 	Total Validation Loss: 260.849142 	 Duration: 41.587
>> Dataset Linguateca:	Training Loss: 28.727354	Validation Loss:260.849142
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (281.256340 --> 260.849142).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.58it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 117.27it/s]

=======================================================================================
Epoch: 11 	 Learning Rate: 1.000	Total Training Loss: 26.892709 	Total Validation Loss: 251.814175 	 Duration: 41.423
>> Dataset Linguateca:	Training Loss: 26.892709	Validation Loss:251.814175
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (260.849142 --> 251.814175).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.15it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 131.35it/s]

=======================================================================================
Epoch: 12 	 Learning Rate: 1.000	Total Training Loss: 25.198681 	Total Validation Loss: 227.035328 	 Duration: 41.169
>> Dataset Linguateca:	Training Loss: 25.198681	Validation Loss:227.035328
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (251.814175 --> 227.035328).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.79it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.37it/s]

=======================================================================================
Epoch: 13 	 Learning Rate: 1.000	Total Training Loss: 23.612888 	Total Validation Loss: 208.997192 	 Duration: 41.063
>> Dataset Linguateca:	Training Loss: 23.612888	Validation Loss:208.997192
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (227.035328 --> 208.997192).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.52it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 131.44it/s]

=======================================================================================
Epoch: 14 	 Learning Rate: 1.000	Total Training Loss: 22.376461 	Total Validation Loss: 209.068137 	 Duration: 41.178
>> Dataset Linguateca:	Training Loss: 22.376461	Validation Loss:209.068137
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.69it/s]

=======================================================================================
Epoch: 15 	 Learning Rate: 1.000	Total Training Loss: 21.134740 	Total Validation Loss: 188.512927 	 Duration: 40.968
>> Dataset Linguateca:	Training Loss: 21.134740	Validation Loss:188.512927
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (208.997192 --> 188.512927).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.09it/s]

=======================================================================================
Epoch: 16 	 Learning Rate: 1.000	Total Training Loss: 19.902998 	Total Validation Loss: 173.332333 	 Duration: 41.109
>> Dataset Linguateca:	Training Loss: 19.902998	Validation Loss:173.332333
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (188.512927 --> 173.332333).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.51it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.93it/s]

=======================================================================================
Epoch: 17 	 Learning Rate: 1.000	Total Training Loss: 19.025399 	Total Validation Loss: 168.257102 	 Duration: 40.840
>> Dataset Linguateca:	Training Loss: 19.025399	Validation Loss:168.257102
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (173.332333 --> 168.257102).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.88it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.85it/s]

=======================================================================================
Epoch: 18 	 Learning Rate: 1.000	Total Training Loss: 18.042516 	Total Validation Loss: 156.835668 	 Duration: 41.338
>> Dataset Linguateca:	Training Loss: 18.042516	Validation Loss:156.835668
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (168.257102 --> 156.835668).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.52it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.07it/s]

=======================================================================================
Epoch: 19 	 Learning Rate: 1.000	Total Training Loss: 17.227052 	Total Validation Loss: 151.589673 	 Duration: 40.879
>> Dataset Linguateca:	Training Loss: 17.227052	Validation Loss:151.589673
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (156.835668 --> 151.589673).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.73it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.99it/s]

=======================================================================================
Epoch: 20 	 Learning Rate: 1.000	Total Training Loss: 16.348542 	Total Validation Loss: 144.281577 	 Duration: 41.020
>> Dataset Linguateca:	Training Loss: 16.348542	Validation Loss:144.281577
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (151.589673 --> 144.281577).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.61it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 118.40it/s]

=======================================================================================
Epoch: 21 	 Learning Rate: 1.000	Total Training Loss: 15.533427 	Total Validation Loss: 140.362638 	 Duration: 41.114
>> Dataset Linguateca:	Training Loss: 15.533427	Validation Loss:140.362638
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (144.281577 --> 140.362638).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.98it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 119.40it/s]

=======================================================================================
Epoch: 22 	 Learning Rate: 1.000	Total Training Loss: 14.808968 	Total Validation Loss: 155.607931 	 Duration: 41.013
>> Dataset Linguateca:	Training Loss: 14.808968	Validation Loss:155.607931
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.14it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.97it/s]

=======================================================================================
Epoch: 23 	 Learning Rate: 1.000	Total Training Loss: 14.265051 	Total Validation Loss: 131.142752 	 Duration: 40.883
>> Dataset Linguateca:	Training Loss: 14.265051	Validation Loss:131.142752
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (140.362638 --> 131.142752).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.92it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.35it/s]

=======================================================================================
Epoch: 24 	 Learning Rate: 1.000	Total Training Loss: 13.697965 	Total Validation Loss: 130.549218 	 Duration: 40.977
>> Dataset Linguateca:	Training Loss: 13.697965	Validation Loss:130.549218
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (131.142752 --> 130.549218).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.85it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.14it/s]

=======================================================================================
Epoch: 25 	 Learning Rate: 1.000	Total Training Loss: 13.142836 	Total Validation Loss: 123.171897 	 Duration: 40.720
>> Dataset Linguateca:	Training Loss: 13.142836	Validation Loss:123.171897
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (130.549218 --> 123.171897).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:38<00:00,  2.88it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.34it/s]

=======================================================================================
Epoch: 26 	 Learning Rate: 1.000	Total Training Loss: 12.609551 	Total Validation Loss: 118.847481 	 Duration: 41.164
>> Dataset Linguateca:	Training Loss: 12.609551	Validation Loss:118.847481
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (123.171897 --> 118.847481).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.94it/s]

=======================================================================================
Epoch: 27 	 Learning Rate: 1.000	Total Training Loss: 12.273646 	Total Validation Loss: 120.685942 	 Duration: 40.842
>> Dataset Linguateca:	Training Loss: 12.273646	Validation Loss:120.685942
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.65it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.08it/s]

=======================================================================================
Epoch: 28 	 Learning Rate: 1.000	Total Training Loss: 11.968161 	Total Validation Loss: 117.589155 	 Duration: 40.925
>> Dataset Linguateca:	Training Loss: 11.968161	Validation Loss:117.589155
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (118.847481 --> 117.589155).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.78it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.31it/s]

=======================================================================================
Epoch: 29 	 Learning Rate: 1.000	Total Training Loss: 11.516924 	Total Validation Loss: 122.479094 	 Duration: 40.684
>> Dataset Linguateca:	Training Loss: 11.516924	Validation Loss:122.479094
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.08it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.24it/s]

=======================================================================================
Epoch: 30 	 Learning Rate: 1.000	Total Training Loss: 10.998120 	Total Validation Loss: 123.774591 	 Duration: 40.897
>> Dataset Linguateca:	Training Loss: 10.998120	Validation Loss:123.774591
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.63it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.08it/s]

=======================================================================================
Epoch: 31 	 Learning Rate: 1.000	Total Training Loss: 10.649548 	Total Validation Loss: 111.873786 	 Duration: 41.046
>> Dataset Linguateca:	Training Loss: 10.649548	Validation Loss:111.873786
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (117.589155 --> 111.873786).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  3.09it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.18it/s]

=======================================================================================
Epoch: 32 	 Learning Rate: 1.000	Total Training Loss: 10.368917 	Total Validation Loss: 122.121948 	 Duration: 40.826
>> Dataset Linguateca:	Training Loss: 10.368917	Validation Loss:122.121948
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.47it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.50it/s]

=======================================================================================
Epoch: 33 	 Learning Rate: 1.000	Total Training Loss: 10.059186 	Total Validation Loss: 107.307159 	 Duration: 40.970
>> Dataset Linguateca:	Training Loss: 10.059186	Validation Loss:107.307159
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (111.873786 --> 107.307159).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.66it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 118.69it/s]

=======================================================================================
Epoch: 34 	 Learning Rate: 1.000	Total Training Loss: 9.714499 	Total Validation Loss: 102.797161 	 Duration: 40.786
>> Dataset Linguateca:	Training Loss: 9.714499	Validation Loss:102.797161
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (107.307159 --> 102.797161).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.70it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.34it/s]

=======================================================================================
Epoch: 35 	 Learning Rate: 1.000	Total Training Loss: 9.413519 	Total Validation Loss: 104.827478 	 Duration: 40.693
>> Dataset Linguateca:	Training Loss: 9.413519	Validation Loss:104.827478
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.64it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.11it/s]

=======================================================================================
Epoch: 36 	 Learning Rate: 1.000	Total Training Loss: 9.144958 	Total Validation Loss: 100.649543 	 Duration: 40.947
>> Dataset Linguateca:	Training Loss: 9.144958	Validation Loss:100.649543
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (102.797161 --> 100.649543).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.78it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.28it/s]

=======================================================================================
Epoch: 37 	 Learning Rate: 1.000	Total Training Loss: 9.159237 	Total Validation Loss: 99.379797 	 Duration: 40.760
>> Dataset Linguateca:	Training Loss: 9.159237	Validation Loss:99.379797
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (100.649543 --> 99.379797).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.76it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.33it/s]

=======================================================================================
Epoch: 38 	 Learning Rate: 1.000	Total Training Loss: 8.897655 	Total Validation Loss: 96.120987 	 Duration: 40.891
>> Dataset Linguateca:	Training Loss: 8.897655	Validation Loss:96.120987
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (99.379797 --> 96.120987).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.83it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.97it/s]

=======================================================================================
Epoch: 39 	 Learning Rate: 1.000	Total Training Loss: 8.546029 	Total Validation Loss: 97.933575 	 Duration: 40.745
>> Dataset Linguateca:	Training Loss: 8.546029	Validation Loss:97.933575
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.78it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.88it/s]

=======================================================================================
Epoch: 40 	 Learning Rate: 1.000	Total Training Loss: 8.289495 	Total Validation Loss: 95.264697 	 Duration: 40.829
>> Dataset Linguateca:	Training Loss: 8.289495	Validation Loss:95.264697
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (96.120987 --> 95.264697).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.69it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.30it/s]

=======================================================================================
Epoch: 41 	 Learning Rate: 1.000	Total Training Loss: 8.054830 	Total Validation Loss: 93.922998 	 Duration: 41.087
>> Dataset Linguateca:	Training Loss: 8.054830	Validation Loss:93.922998
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (95.264697 --> 93.922998).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.77it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.01it/s]

=======================================================================================
Epoch: 42 	 Learning Rate: 1.000	Total Training Loss: 7.877191 	Total Validation Loss: 97.303036 	 Duration: 40.795
>> Dataset Linguateca:	Training Loss: 7.877191	Validation Loss:97.303036
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.56it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 132.08it/s]

=======================================================================================
Epoch: 43 	 Learning Rate: 1.000	Total Training Loss: 7.773182 	Total Validation Loss: 91.354463 	 Duration: 40.784
>> Dataset Linguateca:	Training Loss: 7.773182	Validation Loss:91.354463
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (93.922998 --> 91.354463).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.60it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 133.92it/s]

=======================================================================================
Epoch: 44 	 Learning Rate: 1.000	Total Training Loss: 7.546069 	Total Validation Loss: 95.718921 	 Duration: 40.883
>> Dataset Linguateca:	Training Loss: 7.546069	Validation Loss:95.718921
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.72it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.17it/s]

=======================================================================================
Epoch: 45 	 Learning Rate: 1.000	Total Training Loss: 7.381346 	Total Validation Loss: 92.484949 	 Duration: 40.684
>> Dataset Linguateca:	Training Loss: 7.381346	Validation Loss:92.484949
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.83it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.47it/s]

=======================================================================================
Epoch: 46 	 Learning Rate: 1.000	Total Training Loss: 7.270488 	Total Validation Loss: 90.215623 	 Duration: 40.786
>> Dataset Linguateca:	Training Loss: 7.270488	Validation Loss:90.215623
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (91.354463 --> 90.215623).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.75it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.26it/s]

=======================================================================================
Epoch: 47 	 Learning Rate: 1.000	Total Training Loss: 7.151504 	Total Validation Loss: 90.588169 	 Duration: 40.677
>> Dataset Linguateca:	Training Loss: 7.151504	Validation Loss:90.588169
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.80it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 131.56it/s]

=======================================================================================
Epoch: 48 	 Learning Rate: 1.000	Total Training Loss: 6.969540 	Total Validation Loss: 94.396449 	 Duration: 40.848
>> Dataset Linguateca:	Training Loss: 6.969540	Validation Loss:94.396449
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.62it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.27it/s]

=======================================================================================
Epoch: 49 	 Learning Rate: 1.000	Total Training Loss: 6.758916 	Total Validation Loss: 89.174307 	 Duration: 40.780
>> Dataset Linguateca:	Training Loss: 6.758916	Validation Loss:89.174307
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (90.215623 --> 89.174307).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.67it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.63it/s]

=======================================================================================
Epoch: 50 	 Learning Rate: 1.000	Total Training Loss: 6.523092 	Total Validation Loss: 84.851318 	 Duration: 40.829
>> Dataset Linguateca:	Training Loss: 6.523092	Validation Loss:84.851318
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (89.174307 --> 84.851318).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.87it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.33it/s]

=======================================================================================
Epoch: 51 	 Learning Rate: 1.000	Total Training Loss: 6.494214 	Total Validation Loss: 82.960137 	 Duration: 40.815
>> Dataset Linguateca:	Training Loss: 6.494214	Validation Loss:82.960137
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (84.851318 --> 82.960137).  Saving model ...
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.86it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.52it/s]

=======================================================================================
Epoch: 52 	 Learning Rate: 1.000	Total Training Loss: 6.340250 	Total Validation Loss: 85.031163 	 Duration: 40.783
>> Dataset Linguateca:	Training Loss: 6.340250	Validation Loss:85.031163
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.78it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.41it/s]

=======================================================================================
Epoch: 53 	 Learning Rate: 1.000	Total Training Loss: 6.161212 	Total Validation Loss: 91.133667 	 Duration: 40.652
>> Dataset Linguateca:	Training Loss: 6.161212	Validation Loss:91.133667
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
=======================================================================================
train: batch_size=32, policy=visconde: 100% 104/104 [00:37<00:00,  2.59it/s]
val: batch_size=1, policy=emilia: 100% 419/419 [00:03<00:00, 134.52it/s]

=======================================================================================
Epoch: 54 	 Learning Rate: 1.000	Total Training Loss: 5.986541 	Total Validation Loss: 82.343767 	 Duration: 40.657
>> Dataset Linguateca:	Training Loss: 5.986541	Validation Loss:82.343767
----------------------------------------------------------------------------------------
Comparing loss on ['Linguateca'] dataset(s)
Validation loss decreased (82.960137 --> 82.343767).  Saving model ...
=======================================================================================
test: batch_size=1, policy=emilia: 100% 420/420 [00:03<00:00, 111.38it/s]

Test Accuracy (Overall): 96% (7713/7993)

Test Accuracy (on Linguateca Dataset): 96.50% (7713/7993)
